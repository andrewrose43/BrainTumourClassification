# -*- coding: utf-8 -*-
"""Brain Tumor Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a_nFWBQA3mbrUsxjkL_S0Yl71mPJANEB

**Brain Tumor Classification**

**ECE 470 Project**

Andrew Rose		::	V00884894

Braidon Joe		::	V00822287

Ethan Janus		::	V00855202
"""

# ------------------------------------------------------------------------------
# SECTION 1 :: IMAGE LOADING AND PROCESSING
# ------------------------------------------------------------------------------

# ------------------------------------------------------------------------------
# IMPORTS

import tensorflow as tf
from tensorflow.keras import layers
import os


# ------------------------------------------------------------------------------
# VARIABLES AND CONSTANTS

# Vector of the training and testing folder paths in the ML data
paths = ['/content/Brain-Tumor-Classification-Data/Training',
         '/content/Brain-Tumor-Classification-Data/Testing']

# Image resolution for training and testing images
# This value becomes both the width and height of the downscaled images
# (they're stored as 256x256 in the database)
image_resolution = 128

validation_data_ratio = 0.1
seed = 1126717640


# ------------------------------------------------------------------------------
# IMAGE LOADING AND PROCESSING

# Description:
# Loads the ML data images from a github cloud storage into the colab storage
# space. A tf.keras function is used to create a dataset for the training data
# and the testing data. The images in both of the datasets have greyscale pixel
# values with the range 0-255 so the pixel values are normalized to be a
# floating point value between 0 and 1.

# NOTE:
# Images that are not square are cropped rather then stretched when being
# converted to square images.


# Clone Data from Github
try:
  for i in range(len(paths)):                                                   # Loop through all paths needed for the data
    os.listdir(paths[i])                                                        # Check if each path exists in the IDE's storage
except:
  !git clone https://github.com/Fremic/Brain-Tumor-Classification-Data.git      # Clone the data from the github storage into the colab storage

# Create training and testing datasets unsing tf.keras functions
train_dataset = tf.keras.preprocessing.image_dataset_from_directory(
                                                        paths[0],
                                                        labels='inferred',
                                                        label_mode='categorical',
                                                        color_mode='grayscale',
                                                        image_size=(image_resolution, image_resolution),
                                                        validation_split=validation_data_ratio,
                                                        subset='training',
                                                        seed=seed,
                                                        smart_resize=True
                                                        )

validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(
                                                        paths[0],
                                                        labels='inferred',
                                                        label_mode='categorical',
                                                        color_mode='grayscale',
                                                        image_size=(image_resolution, image_resolution),
                                                        validation_split=validation_data_ratio,
                                                        subset='validation',
                                                        seed=seed,
                                                        smart_resize=True
                                                        )

test_dataset = tf.keras.preprocessing.image_dataset_from_directory(
                                                        paths[1],
                                                        labels='inferred',
                                                        label_mode='categorical',
                                                        color_mode='grayscale',
                                                        image_size=(image_resolution, image_resolution),
                                                        smart_resize=True
                                                        )


# Normalize pixel values for each image to be a decimal value between 0 and 1
# Returns:
# normalized_train_dataset        ::  A tf.keras dataset contianing 90% of the training data with normalized image pixel values
# normalized_validation_dataset   ::  A tf.keras dataset contianing 10% of the training data with normalized image pixel values
# normalized_test_dataset         ::  A tf.keras dataset contianing all the testing data with normalized image pixel values
normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)
normalized_train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))
normalized_validation_dataset = validation_dataset.map(lambda x, y: (normalization_layer(x), y))
normalized_test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), y))

# ------------------------------------------------------------------------------
# SECTION 2 :: IMAGE PLOTTING AND DISPLAY
# ------------------------------------------------------------------------------

# ------------------------------------------------------------------------------
# IMPORTS

import numpy as np
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageOps


# ------------------------------------------------------------------------------
# IMAGE PLOTTING AND DISPLAY

# Description:
# A plot of some sample images in the datasets are displayed to give a
# representation of the data.

# Print an example set of 9 images to show they have been preprocessed
plt.figure(figsize=(10, 10))
for images, labels in train_dataset.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    im = images[i]
    im = im[:,:,0]
    plt.imshow(im.numpy().astype("uint8"))
    plt.axis("off")
plt.savefig('9images.png')

# ------------------------------------------------------------------------------
# SECTION 3 :: FUNCTIONS
# ------------------------------------------------------------------------------

# ------------------------------------------------------------------------------
# IMPORTS

from tensorflow.keras import datasets, layers, models


# ------------------------------------------------------------------------------
# BUILDING THE NETWORK

# Description:
# This cell specifies the layout of the neural network.
# Our network is inspired by the familiar LeNet-5 convolutional neural network, adapted to this context.
# We seek to both a) adapt it to larger images and b) find optimal hyperparameters for it using grid search.
# Specifically, the hyperparameters in question are 1) the size of the convolution kernels and 2) the activation function used in convolution.
# https://www.datasciencecentral.com/profiles/blogs/lenet-5-a-classic-cnn-architecture
# We also use grid search to determine optimal parameters for our network.

def build_model(ks,act):
  # Parameters:
  # ks (int)      ::  Kernel size
  # act (string)  ::  Activation function

  model = models.Sequential()
  model.add(layers.Conv2D(filters=6, kernel_size=(ks,ks), activation=act, input_shape=(image_resolution, image_resolution, 1)))
  model.add(layers.AveragePooling2D(pool_size=(2,2), strides=(2,2)))
  model.add(layers.Conv2D(filters=16, kernel_size=(ks,ks), activation=act))
  model.add(layers.AveragePooling2D(pool_size=(2,2), strides=(2,2)))
  model.add(layers.Conv2D(filters=120, kernel_size=(ks,ks), activation=act))
  model.add(layers.Flatten())                                                   # Flatten the data into 1D format to be accepted by the Dense layers that will get us our output classes
  model.add(layers.Dense(84, activation=act))
  model.add(layers.Dense(4, activation='softmax'))                              # One class for each of 3 tumour types, plus one for no tumour

  model.compile(optimizer='adam',
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
              metrics=['accuracy']
              )

  return model

# ------------------------------------------------------------------------------
# DISPLAY MODEL ARRAY

def print_array(arr,value,precision):
  # Description:
  # The print_array function takes in a history array and prints out an array
  # of either the loss values or the accuracy values.

  # Parameters:
  # arr (array)     ::  History array
  # value (string)  ::  Value from array to be displayed ('loss' or 'accuracy')

  # Create an array of None that is the same size as the given array
  print_arr = [ [ None for x in range(len(arr[0])) ]
                    for y in range(len(arr)) ]
  
  # Create a command string for the precision of the output values
  decimal_places = '.' + str(precision) + 'f'

  # Add the last element in each history array position to the corresponding position in the print out array
  for i in range(len(arr)):
    for j in range(len(arr[i])):
      if print_arr[i][j] != arr[i][j]:
        last_element_index = len(arr[i][j].history[value])
        print_arr[i][j] = format(arr[i][j].history[value][last_element_index-1], decimal_places)

  # Print out each row of the print out array
  for i in range(len(arr)):
    print(print_arr[i])


def print_data(model_active_arr, history_arr, prec):
  print()
  print('model_active_array')
  for i in range(len(model_active_arr)):
    print(model_active_arr[i])
  print()
  print('history_array - loss')
  print_array(history_arr,'loss',prec)
  print()
  print('history_array - accuracy')
  print_array(history_arr,'accuracy',prec)

"""For help choosing an optimizer function: https://mlfromscratch.com/optimizers-explained/"""

# ------------------------------------------------------------------------------
# SECTION 4.1 :: HYPERPARAMETER OPTIMIZATION
# ------------------------------------------------------------------------------

# ------------------------------------------------------------------------------
# IMPORTS

from tensorflow.keras import datasets, layers, models


# ------------------------------------------------------------------------------
# VARIABLES AND CONSTANTS

first_sweep_kernel_size = 2
min_kernel_size = 2
max_kernel_size = 18
num_epochs = 10

# Array of all tf.keras Built-in activation functions
activation_functions = ['elu',
                        'exponential',
                        'gelu',
                        'hard_sigmoid',
                        'linear',
                        'relu',
                        'selu',
                        'sigmoid',
                        'softmax',
                        'softplus',
                        'softsign',
                        'swish',
                        'tanh']

models_array = [ [ None for x in range(max_kernel_size - min_kernel_size + 1) ]
                  for y in range(len(activation_functions)) ]
history_array = [ [ None for x in range(max_kernel_size - min_kernel_size + 1) ]
                   for y in range(len(activation_functions)) ]

model_active_array = [ [ True for x in range(max_kernel_size - min_kernel_size + 1) ]
                      for y in range(len(activation_functions)) ]


# ------------------------------------------------------------------------------
# INITIAL MODEL TRAINING

# Description:

print("TRAINING MODELS   ::   Performing Initial Training")

for act in activation_functions:

  row = activation_functions.index(act)
  column = first_sweep_kernel_size - min_kernel_size
  
  models_array[row][column] = build_model(first_sweep_kernel_size, act)
  print('Training Model   -   Kernel size: ' + str(first_sweep_kernel_size) + '   Activation: ' + act)
  history_array[row][column] = models_array[row][column].fit(x=normalized_train_dataset, validation_data=normalized_validation_dataset, epochs=1)

print_data(model_active_array, history_array, 4)


# ------------------------------------------------------------------------------
# FIRST PASS OF CUTS

print()
print("TRAINING MODELS   ::   Performing First Pass of Cuts")

# Deternine the maximum accuracy model after one itteration
min_loss = 100
max_accuracy = 0
for act in activation_functions:

  row = activation_functions.index(act)
  column = first_sweep_kernel_size - min_kernel_size
  
  # Update the minimum loss value found so far
  if history_array[row][column].history['loss'][0] < min_loss:
    min_loss = history_array[row][column].history['loss'][0]
  
  # Update the maximum accuracy value found so far
  if history_array[row][column].history['accuracy'][0] > max_accuracy:
    max_accuracy = history_array[row][column].history['accuracy'][0]

# Remove the models that had a significantly lower accuracy then the highest
i = 0
while i < len(activation_functions):
  if 2 * history_array[i][column].history['accuracy'][0] < max_accuracy:
    print('Removed: ' + activation_functions[i])
    activation_functions.pop(i)
    models_array.pop(i)
    history_array.pop(i)
    model_active_array.pop(i)
  else:
    i = i + 1


# ------------------------------------------------------------------------------
# MODEL TRAINING

print()
print("TRAINING MODELS   ::   Performing Training")

for epoch in range(2, num_epochs+1):

  print()
  print('------ Performing Training - Epoch ' + str(epoch) + ' ------')

  for act in activation_functions:

    for ks in range(min_kernel_size, max_kernel_size+1):

      row = activation_functions.index(act)
      column = ks - min_kernel_size

      # Build and train missing models in array
      if models_array[row][column] == None:
        models_array[row][column] = build_model(ks, act)
        print('Training Model   -   Kernel size: ' + str(ks) + '   Activation: ' + act)
        history_array[row][column] = models_array[row][column].fit(x=normalized_train_dataset, validation_data=normalized_validation_dataset, epochs=1)
  
      if model_active_array[row][column] == True:
        # Perform another training itteration on each model
        print('Training Model   -   Kernel size: ' + str(ks) + '   Activation: ' + act)
        history = models_array[row][column].fit(x=normalized_train_dataset, validation_data=normalized_validation_dataset, epochs=epoch, initial_epoch=epoch-1)
        history_array[row][column].history['loss'].extend(history.history['loss'])
        history_array[row][column].history['accuracy'].extend(history.history['accuracy'])

        # Update the minimum loss value if a new lowest value exists
        if history_array[row][column].history['loss'][epoch-1] < min_loss:
          min_loss = history_array[row][column].history['loss'][epoch-1]

        # Calculate the lowest possible loss value for a given model
        potential_min_loss = 2*history_array[row][column].history['loss'][epoch-1] - history_array[row][column].history['loss'][epoch-2]
          
        # If there already exists a loss value that is lower then the lowest possible loss value in this model, remove the model from itteration
        if potential_min_loss > min_loss:
          model_active_array[row][column] = False

  print_data(model_active_array, history_array, 4)

# ------------------------------------------------------------------------------
# SECTION 4.2 :: MODEL SELECTION
# ------------------------------------------------------------------------------

# Pick best model from remaining models
min_loss = 100
for i in range(len(models_array)):
  for j in range(len(models_array[i])):
    if model_active_array[i][j] == True:
      if history_array[i][j].history['loss'][num_epochs-1] < min_loss:
        activation_function = activation_functions[i]
        kernel_size = j + min_kernel_size
        model = models_array[i][j]
        history = history_array[i][j]
        min_loss = history_array[i][j].history['loss'][num_epochs-1]

print('Activation Function: ' + activation_function)
print('Kernel Size: ' + str(kernel_size))
print('loss: ' + str(history.history['loss'][num_epochs-1]))
print('accuracy: ' + str(history.history['accuracy'][num_epochs-1]))
model.summary()

# ------------------------------------------------------------------------------
# SECTION 4b :: SIMPLE MODEL BUILDER
# ------------------------------------------------------------------------------

# NOT FOR FINAL VERSION!

kernelSize = 2
activationFunction = 'relu'
numOfEpochs = 10

model = models.Sequential()
model.add(layers.Conv2D(filters=6, kernel_size=kernelSize, activation=activationFunction, input_shape=(image_resolution, image_resolution, 1)))
model.add(layers.AveragePooling2D(pool_size=(2,2), strides=(2,2)))
model.add(layers.Conv2D(filters=16, kernel_size=kernelSize, activation=activationFunction))
model.add(layers.AveragePooling2D(pool_size=(2,2), strides=(2,2)))
model.add(layers.Conv2D(filters=120, kernel_size=kernelSize, activation=activationFunction))
model.add(layers.Flatten())
model.add(layers.Dense(84, activation=activationFunction))
model.add(layers.Dense(4, activation='softmax'))

model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), metrics=['accuracy'])

history = model.fit(x=normalized_train_dataset, validation_data=normalized_validation_dataset, epochs=numOfEpochs)

# ------------------------------------------------------------------------------
# SECTION 5 :: MODEL EVALUATION
# ------------------------------------------------------------------------------

test_eval = model.evaluate(normalized_test_dataset)
print('Test loss:', test_eval[0])
print('Test accuracy:', test_eval[1])

# Testing Findings:
# Epoch values greater then 10 have negligable affect to performance
# 'adam' has a significant increase in performance to 'SGD'

# ------------------------------------------------------------------------------
# SECTION 6 :: MODEL PLOTS
# ------------------------------------------------------------------------------

# Creates plots of training vs validation accuracy and
# training vs validation loss over the 10 epochs of training

accuracy = [100 * n for n in history.history['accuracy']]
val_accuracy = [100 * n for n in history.history['val_accuracy']]
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1,len(accuracy)+1)

plt.plot(epochs, accuracy, 'xb-', label='Training accuracy', color='green')
plt.plot(epochs, val_accuracy, 'xb-', label='Validation accuracy', color='red')
plt.title('Training and validation accuracy (ReLU)')
plt.xlabel('Epochs')
plt.ylabel('Classification Accuracy (%)')
plt.legend()
plt.savefig('acc.png')
plt.figure()
plt.plot(epochs, loss, 'xb-', label='Training loss', color='green')
plt.plot(epochs, val_loss, 'xb-', label='Validation loss', color='red')
plt.title('Training and validation loss (ReLU)')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.savefig('loss.png')
plt.show()